{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspired by\n",
    "# https://github.com/fjg00/Facebook-Group-Post-Scraper/blob/main/Facebook%20Group%20Parser.py\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "import time\n",
    "import json\n",
    "\n",
    "import os\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER DEFINED INPUTS\n",
    "import Login\n",
    "CHROME_DRIVER_PATH = './chromedriver' # must download this yourself\n",
    "\n",
    "# EMAIL, PASSWORD from Login file (explained in README.md)\n",
    "EMAIL = Login.EMAIL\n",
    "PASSWORD = Login.PASSWORD\n",
    "\n",
    "# Group number\n",
    "GROUP_IDS = [1379345962387168, \"kamer.in.eindhoven\"] #USER INPUT\n",
    "GROUP_URLS = []\n",
    "for id in GROUP_IDS:\n",
    "    item = 'https://www.facebook.com/groups/'+str(id)\n",
    "    GROUP_URLS.append(item)\n",
    "\n",
    "SEARCH_PROMPT = \"\" # something to be searched for in the group\n",
    "SEARCH_PROMPT = SEARCH_PROMPT.replace(\" \",\"%20\")\n",
    "\n",
    "# TODO populate this\n",
    "# Copy the group description and put it here to be able to filter it out\n",
    "GROUP_DESCRIPTIONS = [\"\"\"-Kamer in Eindhoven-\n",
    "Kamer in Eindhoven is een platform waar vraag escraper.posts[0]n aanbod (kosteloos) worden samengebracht. Ben je opzoek naar een kamer, huisgenoot of bied je een kamer aan, word dan nu lid van deze facebookgroep.\n",
    "Je scrollt door de nieuwste kamers en huisgenoten om vervolgens direct te reageren naar de aanbieder. Bij ons geen inschrijfkosten, gewoon studenten onder elkaar.\n",
    "Kamer in Eindhoven is onderdeel van de Facebook community Zoekt Kamer inâ€¦ Wij zijn ook actief in Nijmegen, Amsterdam, Delft, Rotterdam, Groningen, Maastricht, Breda, Utrecht, Leiden, Den Haag, Haarlem en Amersfoort.\n",
    "Sinds 2020 hebben wij een samenwerking met de gratis app MyHospi. Hiermee willen wij het proces voor jullie nog makkelijker maken.\n",
    "HOE WERKT MYHOSPI VOOR HUIZEN?\n",
    "1. Plaats de kamer op myHospi via de app.\n",
    "2. Deel de link naar het huis in je FB bericht en geef aan dat mensen via myHospi (de link) moeten reageren. Met myHospi krijg je direct een compleet beeld van de mensen die reageren op jouw kamer. Geen onpersoonlijke mails maar complete profielen.\n",
    "3. Swipe en Like!\n",
    "4. Nodig kandidaten uit en deel de link naar je videocall.\"\"\"] # User input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FBScraper import FBScraper\n",
    "scraper = FBScraper(CHROME_DRIVER_PATH, Login.EMAIL, Login.PASSWORD)\n",
    "scraper.openAndLogin(hide=True)\n",
    "scraper.setGroupInfo(GROUP_IDS)\n",
    "scraper.getPostURLs(limit=5, group_index=0)\n",
    "print(scraper.posts[0])\n",
    "test_profile = scraper.getPosterURL(scraper.posts[0])\n",
    "test_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper.closeSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addvalues(dictionary: dict, key, L: list):\n",
    "    \"\"\"Append multiple values to a key in the given dictionary\"\"\"\n",
    "    if key not in dictionary:\n",
    "        dictionary[key] = list()\n",
    "    dictionary[key].extend(L)\n",
    "    return dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opens FB group with search parameters\n",
    "# driver.get(GROUP_URL+SEARCH_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Selection criteria\n",
    "# text = \"\"\n",
    "# comments = \"\"\n",
    "# z = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPosterURL(post: str, group_id: int, page_source=None, soup=None):\n",
    "    driver.get(post)\n",
    "    time.sleep(1)\n",
    "    # TODO something here is broken. Fix this.\n",
    "    poster_class = \"x1i10hfl xjbqb8w x6umtig x1b1mbwd xaqea5y xav7gou x9f619 x1ypdohk xt0psk2 xe8uvvx xdj266r x11i5rnm xat24cr x1mh8g0r xexx8yu x4uap5 x18d9i69 xkhd6sd x16tdsg8 x1hl2dhg xggy1nq x1a2a7pz xt0b8zv xzsf02u x1s688f\"\n",
    "\n",
    "    link = driver.page_source if page_source is None else page_source\n",
    "    page_source = 'https://www.facebook.com' + page_source if link is None else link\n",
    "\n",
    "\n",
    "    soup = BeautifulSoup(page_source, 'html.parser') if soup is None else soup\n",
    "\n",
    "    urls = soup.find_all('a', class_=poster_class)\n",
    "    urls = [url['href'] for url in urls]\n",
    "\n",
    "    identifier = \"/groups/\" + str(group_id)\n",
    "    try:\n",
    "        url = [url for url in urls if identifier in url][0]\n",
    "    except:\n",
    "        url = [url for url in urls if identifier in url]    \n",
    "    url = \"https://www.facebook.com\" + str(url)\n",
    "\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPosterList(post_list: list, group_url: str):\n",
    "    urls = []\n",
    "    for count, post in enumerate(post_list):\n",
    "        urls.append(getPosterURL(post, group_url))\n",
    "        if count % 10:\n",
    "            print (count, ' posters gotten')\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLastXPosters(X: int, group_url):\n",
    "    tolerance_diff = 5\n",
    "    posts = getPostURLs(X+tolerance_diff, group_url)\n",
    "    \n",
    "    now = datetime.now()\n",
    "    now = now.strftime(\"%Y-%m-%d_%H-%M\")\n",
    "    \n",
    "    # Save before continuing\n",
    "    pdf = pd.DataFrame({'post': posts})\n",
    "    pdf.to_csv(f'{now}_{X}_posts.csv', index=False)\n",
    "\n",
    "    posters = getPosterList(posts, group_url)\n",
    "    \n",
    "    pdf = pd.DataFrame({'profiles': posters})\n",
    "    pdf.to_csv(f'{now}_{X}_profiles.csv', index=False)\n",
    "\n",
    "    return posters, posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posters, posts = getLastXPosters(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def message(profile: str, msg: str):\n",
    "    # Add enter to message so that it gets sent\n",
    "    msg_end = msg[len(msg)-2] + msg[len(msg)-1]\n",
    "    if msg_end != \"\\n\":\n",
    "        msg = msg + \"\\n\"\n",
    "\n",
    "    # Open the profile\n",
    "    driver.get(profile)\n",
    "    \n",
    "    # Click the \"Message\" button\n",
    "    xpath_msg_btn = \"//span[contains(text(),'Message')]\"\n",
    "    page_source = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, xpath_msg_btn))).click()\n",
    "\n",
    "    # Write something in the input\n",
    "    xpath_input = \"//p[@class='xdj266r xat24cr']\"\n",
    "    input_field = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, xpath_input)))\n",
    "    input_field.send_keys(msg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getName(profile: str):\n",
    "    # Open the profile link\n",
    "    driver.get(profile)\n",
    "    name_selector = \".x14z4hjw > div:nth-child(1) > h1\"\n",
    "    name = driver.find_element(By.CSS_SELECTOR, name_selector)\n",
    "    return name.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the scraper\n",
    "posters = []\n",
    "posts = []\n",
    "for group in GROUP_URLS:\n",
    "    driver.get(group)\n",
    "    posters_, posts_ = getLastXPosters(10, group)\n",
    "    posters.extend(posters_)\n",
    "    posts.extend(posts_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get names\n",
    "# names = []\n",
    "# for profile in posters:\n",
    "#     print(profile)\n",
    "#     # names.extend(getName(profile))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_df = pd.DataFrame({'names': names, 'profiles': posters, 'posts': posts})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Grab post and comment data\n",
    "# # TODO make nice df out of it\n",
    "# # TODO grab info about posters and commenters\n",
    "# # TODO grab time data (and time difference between post and comment)\n",
    "\n",
    "\n",
    "# WrapperDict = list() \n",
    "# texts = []\n",
    "# translatedBools = []\n",
    "# commentCounts = []\n",
    "\n",
    "# for postCount, a in enumerate(posts):\n",
    "#         driver.get(a)\n",
    "#         time.sleep(1)\n",
    "        \n",
    "#         page_source = driver.page_source\n",
    "#         soup = BeautifulSoup(page_source, 'html.parser')\n",
    "        \n",
    "#         # Relevant classes for text (or translate button)\n",
    "#         translateBtnText = \"x1i10hfl xjbqb8w x6umtig x1b1mbwd xaqea5y xav7gou x9f619 x1ypdohk xt0psk2 xe8uvvx xdj266r x11i5rnm xat24cr x1mh8g0r xexx8yu x4uap5 x18d9i69 xkhd6sd x16tdsg8 x1hl2dhg xggy1nq x1o1ewxj x3x9cwd x1e5q0jg x13rtm0m x1n2onr6 x87ps6o x1a2a7pz xt0b8zv\"\n",
    "#         translateBtn = soup.find('div', class_ = translateBtnText)\n",
    "#         translatedTextClass = \"x193iq5w xeuugli x13faqbe x1vvkbs x10flsy6 x6prxxf xvq8zen xo1l8bm xzsf02u x1yc453h\"\n",
    "#         originalTextClass = \"x193iq5w xeuugli x13faqbe x1vvkbs x10flsy6 x1lliihq x1s928wv xhkezso x1gmr53x x1cpjm7i x1fgarty x1943h6x x4zkp8e x41vudc x6prxxf xvq8zen xo1l8bm xzsf02u x1yc453h\"\n",
    "\n",
    "#         if (translateBtn is not None) and (soup.find('div', class_ = translateBtnText).text == \"See Translation\"):\n",
    "#                 isTranslated = False\n",
    "#                 # Original\n",
    "#                 # text = soup.find('span' , class_ = originalTextClass).text #post\n",
    "#                 t = soup.find('div', class_ = \"x1swvt13 x1l90r2v x1pi30zi x1iorvi4\")\n",
    "#                 if t is not None:\n",
    "#                         text = t.text #post\n",
    "#                 else:\n",
    "#                         # This may result in text=\"Facebook\", could find all of this class and choose one where it doesn't equal Facebook\n",
    "#                         text = soup.find('span' , class_ = originalTextClass).text #post\n",
    "#         elif soup.find('span', class_ = translatedTextClass) is not None:\n",
    "#                 # Translated\n",
    "#                 text = soup.find('span' , class_ = translatedTextClass).text #post\n",
    "#                 isTranslated = True\n",
    "#         elif soup.find('span', class_ = originalTextClass) is not None:\n",
    "#                 # Original English\n",
    "#                 text = soup.find('span', class_ = originalTextClass).text #post\n",
    "#                 isTranslated = False\n",
    "\n",
    "#         # isTranslated = len(soup.find_all('span', class_ = \"x193iq5w xeuugli x13faqbe x1vvkbs x10flsy6 x1pg5gke xvq8zen xo1l8bm x1qq9wsj x1yc453h\")) > 0\n",
    "#         # TODO instead of length check that it equals expected text ('See original' or 'Rate this translation')\n",
    "\n",
    "#         commentCount = soup.find('span', class_ = \"x193iq5w xeuugli x13faqbe x1vvkbs x10flsy6 x1lliihq x1s928wv xhkezso x1gmr53x x1cpjm7i x1fgarty x1943h6x x4zkp8e x41vudc x6prxxf xvq8zen xo1l8bm xi81zsa\").text\n",
    "#         commentCount = int(re.search(r'\\d+', commentCount).group())\n",
    "        \n",
    "#         # TODO create DF/dictionary/smth with all this info\n",
    "#         texts.append(text)\n",
    "#         translatedBools.append(isTranslated)\n",
    "#         commentCounts.append(commentCount)\n",
    "#         if postCount % 5 == 0:\n",
    "#                 print (postCount)\n",
    "\n",
    "#         # w = 1\n",
    "        \n",
    "#         # BranchDict = dict()\n",
    "#         # BranchDict[\"tag\"] = 1\n",
    "#         # BranchDict[\"patterns\"] = 1\n",
    "#         # BranchDict[\"responses\"] = list()\n",
    "#         # L = list()\n",
    "        \n",
    "#         # for post in actualPosts:\n",
    "#         #     s = post.get_text()\n",
    "#         #     # if (s == \"INSERT GROUP DESCRIPTION HERE\"): #USER INPUT\n",
    "#         #     if (s == GROUP_DESCRIPTION): #USER INPUT\n",
    "#         #         time.sleep(1)\n",
    "#         #     elif len(s.split()) < 4:\n",
    "#         #         time.sleep(1)\n",
    "#         #     else:\n",
    "#         #         if w == 1: #post\n",
    "#         #             text= text + post.get_text() \n",
    "#         #             BranchDict[\"tag\"] = SEARCH_PROMPT + str(z) \n",
    "#         #             BranchDict[\"patterns\"] = text\n",
    "#         #             w = w + text1\n",
    "#         #         elif w > 1 : #comments\n",
    "#         #             comments = post.get_text()\n",
    "#         #             L.append(comments)\n",
    "#         #             comments = \"\"\n",
    "#         # z = z + 1\n",
    "#         # addvalues(BranchDict, \"responses\", L)\n",
    "#         # WrapperDict.append(BranchDict)\n",
    "#         # text = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the headless session (to avoid it staying alive in memory)\n",
    "# driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # df = pd.DataFrame([texts, translatedBools, commentCounts], columns=['Text', 'Is_Translated', 'Comment_Count'])\n",
    "# df = pd.DataFrame.from_dict({\"Text\": texts, \"Is_Translated\": translatedBools, \"Comment_Count\": commentCounts, 'URL': list(posts)})\n",
    "\n",
    "# # # Flatten text if necessary\n",
    "# # def flatten(text):\n",
    "# #     if text is list:\n",
    "# #         text = ''.join(text)\n",
    "# #         print (text)\n",
    "# #     return text\n",
    "# # df['Text'] = df['Text'].apply(flatten)\n",
    "# # df.iloc[0]['URL']\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save it, brother\n",
    "# df.to_csv(path_or_buf='scraped_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "69eddf76122a7a77fa761679cc76164f49f300d238d45f30943e7d02df2c26d3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
